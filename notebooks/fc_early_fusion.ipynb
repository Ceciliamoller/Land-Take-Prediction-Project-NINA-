{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190bb051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# add repo root\n",
    "root = Path().resolve().parents[0]\n",
    "sys.path.append(str(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fbc669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"WANDB_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1547d026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from src.data.timeseries_dataset import TimeSeriesDataset\n",
    "from src.config import SENTINEL_DIR, MASK_DIR\n",
    "\n",
    "all_sentinel_files = list(SENTINEL_DIR.glob(\"*_RGBNIRRSWIRQ_Mosaic.tif\"))\n",
    "train_ids, val_ids = train_test_split(all_sentinel_files, test_size=0.1, random_state=0)\n",
    "\n",
    "print(train_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be11fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.transform import ComposeTS, NormalizeBy, RandomCropTS, CenterCropTS\n",
    "\n",
    "CROP = 64  # use 64 if your tiles are tiny; 128 or 256 only if your tiles are large enough\n",
    "\n",
    "train_transform = ComposeTS([\n",
    "    NormalizeBy(10000.0),\n",
    "    RandomCropTS(CROP),\n",
    "])\n",
    "\n",
    "val_transform = ComposeTS([\n",
    "    NormalizeBy(10000.0),\n",
    "    CenterCropTS(CROP),          # deterministic for validation\n",
    "])\n",
    "\n",
    "train_ds = TimeSeriesDataset(train_ids, sensor=\"sentinel\", slice_mode=\"first_half\", transform=train_transform)\n",
    "val_ds   = TimeSeriesDataset(val_ids,   sensor=\"sentinel\", slice_mode=\"first_half\", transform=val_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb30df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=4, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8314d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x, mask = next(iter(train_loader))\n",
    "print(torch.unique(mask))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939eed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ds_raw = TimeSeriesDataset(train_ids, sensor=\"sentinel\", slice_mode=\"first_half\", transform=None)\n",
    "has_zero = False\n",
    "has_nonzero = False\n",
    "\n",
    "for i in range(len(ds_raw)):\n",
    "    _, m = ds_raw[i]\n",
    "    u = torch.unique(m)\n",
    "    if (u == 0).any():\n",
    "        has_zero = True\n",
    "    if (u > 0).any():\n",
    "        has_nonzero = True\n",
    "\n",
    "print(\"any tiles with background (0)?\", has_zero)\n",
    "print(\"any tiles with change (>0)?\", has_nonzero)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dbbd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.models.external.torchrs_fc_cd import FCEF\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# probe batch\n",
    "sample_x, _ = next(iter(train_loader))\n",
    "_, T, C, H, W = sample_x.shape\n",
    "\n",
    "model = FCEF(channels=C, t=T, num_classes=2).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12da63af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def compute_confusion_binary(y_pred, y_true, positive_class=1):\n",
    "    \"\"\"\n",
    "    y_pred, y_true: (B, H, W) with 0/1 labels\n",
    "    returns TP, FP, TN, FN as scalars\n",
    "    \"\"\"\n",
    "    y_pred = (y_pred == positive_class)\n",
    "    y_true = (y_true == positive_class)\n",
    "\n",
    "    tp = (y_pred & y_true).sum().item()\n",
    "    fp = (y_pred & ~y_true).sum().item()\n",
    "    tn = (~y_pred & ~y_true).sum().item()\n",
    "    fn = (~y_pred & y_true).sum().item()\n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "def compute_metrics_from_confusion(tp, fp, tn, fn, eps=1e-8):\n",
    "    accuracy  = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall    = tp / (tp + fn + eps)\n",
    "    f1        = 2 * precision * recall / (precision + recall + eps)\n",
    "    iou       = tp / (tp + fp + fn + eps)  # IoU for the positive class\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"iou\": iou,\n",
    "    }\n",
    "\n",
    "def compute_batch_metrics(logits, mask, positive_class=1):\n",
    "    \"\"\"\n",
    "    logits: (B, 2, H, W)\n",
    "    mask:   (B, H, W) with {0,1}\n",
    "    returns metrics dict for the whole batch\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        pred = torch.argmax(logits, dim=1)  # (B, H, W)\n",
    "        tp, fp, tn, fn = compute_confusion_binary(pred, mask, positive_class)\n",
    "        return compute_metrics_from_confusion(tp, fp, tn, fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17137b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.external.torchrs_fc_cd import FCEF  # or your local file\n",
    "\n",
    "model1 = FCEF(channels=C, t=T, num_classes=2).to(device)  # C,T from a batch\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "x, mask = next(iter(train_loader))\n",
    "x = x.to(device)\n",
    "mask = mask.to(device)\n",
    "\n",
    "model1.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model1(x)\n",
    "    print(\"logits NaN:\", torch.isnan(logits).any().item())\n",
    "    loss = criterion(logits, mask)\n",
    "    print(\"single-batch loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bf5891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "# Start a new wandb run to track this script.\n",
    "run = wandb.init(\n",
    "    # Set the wandb entity where your project will be logged (generally your team name).\n",
    "    entity=\"nina_prosjektoppgave\",\n",
    "    # Set the wandb project where this run will be logged.\n",
    "    project=\"FCEarlyFusion\",\n",
    "    # Track hyperparameters and run metadata.\n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"architecture\": \"FCEF\",\n",
    "        \"dataset\": \"sentinel\",\n",
    "        \"epochs\": 10,\n",
    "    },\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for x, mask in tqdm(train_loader, desc=f\"epoch {epoch+1}\"):\n",
    "        x = x.to(device)          # (B, T, C, H, W)\n",
    "        mask = mask.to(device)    # (B, H, W)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            logits = model(x)         # (B, 2, H, W)\n",
    "            loss = criterion(logits, mask)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, mask in val_loader:\n",
    "            x = x.to(device)\n",
    "            mask = mask.to(device)\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits, mask)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            tp, fp, tn, fn = compute_confusion_binary(pred, mask, positive_class=1)\n",
    "            sum_tp += tp\n",
    "            sum_fp += fp\n",
    "            sum_tn += tn\n",
    "            sum_fn += fn\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_metrics = compute_metrics_from_confusion(sum_tp, sum_fp, sum_tn, sum_fn)\n",
    "\n",
    "  \n",
    "\n",
    "    run.log({\"avg_train_loss\": avg_train_loss,\n",
    "             \"avg_val_loss\": avg_val_loss,\n",
    "             \"IoU\": val_metrics['iou'],\n",
    "             \"F1\": val_metrics['f1'],\n",
    "             \"Precision\": val_metrics['precision'],\n",
    "             \"Recall\": val_metrics['recall'],\n",
    "             \"Accuracy\": val_metrics['accuracy']})\n",
    "\n",
    "    print(\n",
    "        f\"epoch {epoch+1}: \"\n",
    "        f\"train={avg_train_loss:.4f} \"\n",
    "        f\"val={avg_val_loss:.4f} \"\n",
    "        f\"IoU={val_metrics['iou']:.4f} \"\n",
    "        f\"F1={val_metrics['f1']:.4f} \"\n",
    "        f\"Prec={val_metrics['precision']:.4f} \"\n",
    "        f\"Rec={val_metrics['recall']:.4f} \"\n",
    "        f\"Acc={val_metrics['accuracy']:.4f}\"\n",
    "    )\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568177b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_batch(model, data_loader, device, num_examples=3):\n",
    "    model.eval()\n",
    "    x, mask = next(iter(data_loader))  # one batch\n",
    "    x = x.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "        pred = torch.argmax(logits, dim=1)  # (B, H, W)\n",
    "\n",
    "    # move to cpu for plotting\n",
    "    x_cpu = x.cpu()\n",
    "    mask_cpu = mask.cpu()\n",
    "    pred_cpu = pred.cpu()\n",
    "\n",
    "    B, T, C, H, W = x_cpu.shape\n",
    "    num_examples = min(num_examples, B)\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        # very simple \"input\" visualization:\n",
    "        # take last time step, first 3 channels and normalize to [0,1]\n",
    "        img = x_cpu[i, -1, :3]  # (3, H, W)  (if fewer than 3 bands, slice accordingly)\n",
    "        img_min = img.min()\n",
    "        img_max = img.max()\n",
    "        img_vis = (img - img_min) / (img_max - img_min + 1e-8)\n",
    "        img_vis = img_vis.permute(1, 2, 0)  # (H, W, 3)\n",
    "\n",
    "        gt = mask_cpu[i]   # (H, W)\n",
    "        pr = pred_cpu[i]   # (H, W)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(10, 4))\n",
    "        axes[0].imshow(img_vis)\n",
    "        axes[0].set_title(\"Input (t_last, RGB-ish)\")\n",
    "        axes[0].axis(\"off\")\n",
    "\n",
    "        axes[1].imshow(gt, vmin=0, vmax=1)\n",
    "        axes[1].set_title(\"Ground truth mask\")\n",
    "        axes[1].axis(\"off\")\n",
    "\n",
    "        axes[2].imshow(pr, vmin=0, vmax=1)\n",
    "        axes[2].set_title(\"Predicted mask\")\n",
    "        axes[2].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06faf952",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_batch(model, val_loader, device, num_examples=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prosjekt_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
